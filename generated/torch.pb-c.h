/* Generated by the protocol buffer compiler.  DO NOT EDIT! */
/* Generated from: torch.proto */

#ifndef PROTOBUF_C_torch_2eproto__INCLUDED
#define PROTOBUF_C_torch_2eproto__INCLUDED

#include <protobuf-c/protobuf-c.h>

PROTOBUF_C__BEGIN_DECLS

#if PROTOBUF_C_VERSION_NUMBER < 1003000
# error This file was generated by a newer version of protoc-c which is incompatible with your libprotobuf-c headers. Please update your headers.
#elif 1003003 < PROTOBUF_C_MIN_COMPILER_VERSION
# error This file was generated by an older version of protoc-c which is incompatible with your libprotobuf-c headers. Please regenerate this file with a newer version of protoc-c.
#endif


typedef struct _Vaccel__Torch__Tensor Vaccel__Torch__Tensor;
typedef struct _Vaccel__Torch__ModelLoadRequest Vaccel__Torch__ModelLoadRequest;
typedef struct _Vaccel__Torch__ModelRunRequest Vaccel__Torch__ModelRunRequest;
typedef struct _Vaccel__Torch__ModelRunResponse Vaccel__Torch__ModelRunResponse;


/* --- enums --- */

typedef enum _Vaccel__Torch__DataType {
  /*
   * For vAccel value compatibility
   */
  VACCEL__TORCH__DATA_TYPE__UNUSED = 0,
  VACCEL__TORCH__DATA_TYPE__BYTE = 1,
  VACCEL__TORCH__DATA_TYPE__CHAR = 2,
  VACCEL__TORCH__DATA_TYPE__SHORT = 3,
  VACCEL__TORCH__DATA_TYPE__INT = 4,
  VACCEL__TORCH__DATA_TYPE__LONG = 5,
  VACCEL__TORCH__DATA_TYPE__HALF = 6,
  VACCEL__TORCH__DATA_TYPE__FLOAT = 7
    PROTOBUF_C__FORCE_ENUM_TO_BE_INT_SIZE(VACCEL__TORCH__DATA_TYPE)
} Vaccel__Torch__DataType;

/* --- messages --- */

struct  _Vaccel__Torch__Tensor
{
  ProtobufCMessage base;
  ProtobufCBinaryData data;
  size_t n_dims;
  int64_t *dims;
  Vaccel__Torch__DataType type;
};
#define VACCEL__TORCH__TENSOR__INIT \
 { PROTOBUF_C_MESSAGE_INIT (&vaccel__torch__tensor__descriptor) \
    , {0,NULL}, 0,NULL, VACCEL__TORCH__DATA_TYPE__UNUSED }


struct  _Vaccel__Torch__ModelLoadRequest
{
  ProtobufCMessage base;
  int64_t session_id;
  int64_t model_id;
};
#define VACCEL__TORCH__MODEL_LOAD_REQUEST__INIT \
 { PROTOBUF_C_MESSAGE_INIT (&vaccel__torch__model_load_request__descriptor) \
    , 0, 0 }


struct  _Vaccel__Torch__ModelRunRequest
{
  ProtobufCMessage base;
  int64_t session_id;
  int64_t model_id;
  /*
   * optional
   */
  ProtobufCBinaryData run_options;
  size_t n_in_tensors;
  Vaccel__Torch__Tensor **in_tensors;
  uint64_t nr_out_tensors;
};
#define VACCEL__TORCH__MODEL_RUN_REQUEST__INIT \
 { PROTOBUF_C_MESSAGE_INIT (&vaccel__torch__model_run_request__descriptor) \
    , 0, 0, {0,NULL}, 0,NULL, 0 }


struct  _Vaccel__Torch__ModelRunResponse
{
  ProtobufCMessage base;
  size_t n_out_tensors;
  Vaccel__Torch__Tensor **out_tensors;
};
#define VACCEL__TORCH__MODEL_RUN_RESPONSE__INIT \
 { PROTOBUF_C_MESSAGE_INIT (&vaccel__torch__model_run_response__descriptor) \
    , 0,NULL }


/* Vaccel__Torch__Tensor methods */
void   vaccel__torch__tensor__init
                     (Vaccel__Torch__Tensor         *message);
size_t vaccel__torch__tensor__get_packed_size
                     (const Vaccel__Torch__Tensor   *message);
size_t vaccel__torch__tensor__pack
                     (const Vaccel__Torch__Tensor   *message,
                      uint8_t             *out);
size_t vaccel__torch__tensor__pack_to_buffer
                     (const Vaccel__Torch__Tensor   *message,
                      ProtobufCBuffer     *buffer);
Vaccel__Torch__Tensor *
       vaccel__torch__tensor__unpack
                     (ProtobufCAllocator  *allocator,
                      size_t               len,
                      const uint8_t       *data);
void   vaccel__torch__tensor__free_unpacked
                     (Vaccel__Torch__Tensor *message,
                      ProtobufCAllocator *allocator);
/* Vaccel__Torch__ModelLoadRequest methods */
void   vaccel__torch__model_load_request__init
                     (Vaccel__Torch__ModelLoadRequest         *message);
size_t vaccel__torch__model_load_request__get_packed_size
                     (const Vaccel__Torch__ModelLoadRequest   *message);
size_t vaccel__torch__model_load_request__pack
                     (const Vaccel__Torch__ModelLoadRequest   *message,
                      uint8_t             *out);
size_t vaccel__torch__model_load_request__pack_to_buffer
                     (const Vaccel__Torch__ModelLoadRequest   *message,
                      ProtobufCBuffer     *buffer);
Vaccel__Torch__ModelLoadRequest *
       vaccel__torch__model_load_request__unpack
                     (ProtobufCAllocator  *allocator,
                      size_t               len,
                      const uint8_t       *data);
void   vaccel__torch__model_load_request__free_unpacked
                     (Vaccel__Torch__ModelLoadRequest *message,
                      ProtobufCAllocator *allocator);
/* Vaccel__Torch__ModelRunRequest methods */
void   vaccel__torch__model_run_request__init
                     (Vaccel__Torch__ModelRunRequest         *message);
size_t vaccel__torch__model_run_request__get_packed_size
                     (const Vaccel__Torch__ModelRunRequest   *message);
size_t vaccel__torch__model_run_request__pack
                     (const Vaccel__Torch__ModelRunRequest   *message,
                      uint8_t             *out);
size_t vaccel__torch__model_run_request__pack_to_buffer
                     (const Vaccel__Torch__ModelRunRequest   *message,
                      ProtobufCBuffer     *buffer);
Vaccel__Torch__ModelRunRequest *
       vaccel__torch__model_run_request__unpack
                     (ProtobufCAllocator  *allocator,
                      size_t               len,
                      const uint8_t       *data);
void   vaccel__torch__model_run_request__free_unpacked
                     (Vaccel__Torch__ModelRunRequest *message,
                      ProtobufCAllocator *allocator);
/* Vaccel__Torch__ModelRunResponse methods */
void   vaccel__torch__model_run_response__init
                     (Vaccel__Torch__ModelRunResponse         *message);
size_t vaccel__torch__model_run_response__get_packed_size
                     (const Vaccel__Torch__ModelRunResponse   *message);
size_t vaccel__torch__model_run_response__pack
                     (const Vaccel__Torch__ModelRunResponse   *message,
                      uint8_t             *out);
size_t vaccel__torch__model_run_response__pack_to_buffer
                     (const Vaccel__Torch__ModelRunResponse   *message,
                      ProtobufCBuffer     *buffer);
Vaccel__Torch__ModelRunResponse *
       vaccel__torch__model_run_response__unpack
                     (ProtobufCAllocator  *allocator,
                      size_t               len,
                      const uint8_t       *data);
void   vaccel__torch__model_run_response__free_unpacked
                     (Vaccel__Torch__ModelRunResponse *message,
                      ProtobufCAllocator *allocator);
/* --- per-message closures --- */

typedef void (*Vaccel__Torch__Tensor_Closure)
                 (const Vaccel__Torch__Tensor *message,
                  void *closure_data);
typedef void (*Vaccel__Torch__ModelLoadRequest_Closure)
                 (const Vaccel__Torch__ModelLoadRequest *message,
                  void *closure_data);
typedef void (*Vaccel__Torch__ModelRunRequest_Closure)
                 (const Vaccel__Torch__ModelRunRequest *message,
                  void *closure_data);
typedef void (*Vaccel__Torch__ModelRunResponse_Closure)
                 (const Vaccel__Torch__ModelRunResponse *message,
                  void *closure_data);

/* --- services --- */


/* --- descriptors --- */

extern const ProtobufCEnumDescriptor    vaccel__torch__data_type__descriptor;
extern const ProtobufCMessageDescriptor vaccel__torch__tensor__descriptor;
extern const ProtobufCMessageDescriptor vaccel__torch__model_load_request__descriptor;
extern const ProtobufCMessageDescriptor vaccel__torch__model_run_request__descriptor;
extern const ProtobufCMessageDescriptor vaccel__torch__model_run_response__descriptor;

PROTOBUF_C__END_DECLS


#endif  /* PROTOBUF_C_torch_2eproto__INCLUDED */
