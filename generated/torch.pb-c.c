/* Generated by the protocol buffer compiler.  DO NOT EDIT! */
/* Generated from: torch.proto */

/* Do not generate deprecated warnings for self */
#ifndef PROTOBUF_C__NO_DEPRECATED
#define PROTOBUF_C__NO_DEPRECATED
#endif

#include "torch.pb-c.h"
void   vaccel__torch__tensor__init
                     (Vaccel__Torch__Tensor         *message)
{
  static const Vaccel__Torch__Tensor init_value = VACCEL__TORCH__TENSOR__INIT;
  *message = init_value;
}
size_t vaccel__torch__tensor__get_packed_size
                     (const Vaccel__Torch__Tensor *message)
{
  assert(message->base.descriptor == &vaccel__torch__tensor__descriptor);
  return protobuf_c_message_get_packed_size ((const ProtobufCMessage*)(message));
}
size_t vaccel__torch__tensor__pack
                     (const Vaccel__Torch__Tensor *message,
                      uint8_t       *out)
{
  assert(message->base.descriptor == &vaccel__torch__tensor__descriptor);
  return protobuf_c_message_pack ((const ProtobufCMessage*)message, out);
}
size_t vaccel__torch__tensor__pack_to_buffer
                     (const Vaccel__Torch__Tensor *message,
                      ProtobufCBuffer *buffer)
{
  assert(message->base.descriptor == &vaccel__torch__tensor__descriptor);
  return protobuf_c_message_pack_to_buffer ((const ProtobufCMessage*)message, buffer);
}
Vaccel__Torch__Tensor *
       vaccel__torch__tensor__unpack
                     (ProtobufCAllocator  *allocator,
                      size_t               len,
                      const uint8_t       *data)
{
  return (Vaccel__Torch__Tensor *)
     protobuf_c_message_unpack (&vaccel__torch__tensor__descriptor,
                                allocator, len, data);
}
void   vaccel__torch__tensor__free_unpacked
                     (Vaccel__Torch__Tensor *message,
                      ProtobufCAllocator *allocator)
{
  if(!message)
    return;
  assert(message->base.descriptor == &vaccel__torch__tensor__descriptor);
  protobuf_c_message_free_unpacked ((ProtobufCMessage*)message, allocator);
}
void   vaccel__torch__model_load_request__init
                     (Vaccel__Torch__ModelLoadRequest         *message)
{
  static const Vaccel__Torch__ModelLoadRequest init_value = VACCEL__TORCH__MODEL_LOAD_REQUEST__INIT;
  *message = init_value;
}
size_t vaccel__torch__model_load_request__get_packed_size
                     (const Vaccel__Torch__ModelLoadRequest *message)
{
  assert(message->base.descriptor == &vaccel__torch__model_load_request__descriptor);
  return protobuf_c_message_get_packed_size ((const ProtobufCMessage*)(message));
}
size_t vaccel__torch__model_load_request__pack
                     (const Vaccel__Torch__ModelLoadRequest *message,
                      uint8_t       *out)
{
  assert(message->base.descriptor == &vaccel__torch__model_load_request__descriptor);
  return protobuf_c_message_pack ((const ProtobufCMessage*)message, out);
}
size_t vaccel__torch__model_load_request__pack_to_buffer
                     (const Vaccel__Torch__ModelLoadRequest *message,
                      ProtobufCBuffer *buffer)
{
  assert(message->base.descriptor == &vaccel__torch__model_load_request__descriptor);
  return protobuf_c_message_pack_to_buffer ((const ProtobufCMessage*)message, buffer);
}
Vaccel__Torch__ModelLoadRequest *
       vaccel__torch__model_load_request__unpack
                     (ProtobufCAllocator  *allocator,
                      size_t               len,
                      const uint8_t       *data)
{
  return (Vaccel__Torch__ModelLoadRequest *)
     protobuf_c_message_unpack (&vaccel__torch__model_load_request__descriptor,
                                allocator, len, data);
}
void   vaccel__torch__model_load_request__free_unpacked
                     (Vaccel__Torch__ModelLoadRequest *message,
                      ProtobufCAllocator *allocator)
{
  if(!message)
    return;
  assert(message->base.descriptor == &vaccel__torch__model_load_request__descriptor);
  protobuf_c_message_free_unpacked ((ProtobufCMessage*)message, allocator);
}
void   vaccel__torch__model_run_request__init
                     (Vaccel__Torch__ModelRunRequest         *message)
{
  static const Vaccel__Torch__ModelRunRequest init_value = VACCEL__TORCH__MODEL_RUN_REQUEST__INIT;
  *message = init_value;
}
size_t vaccel__torch__model_run_request__get_packed_size
                     (const Vaccel__Torch__ModelRunRequest *message)
{
  assert(message->base.descriptor == &vaccel__torch__model_run_request__descriptor);
  return protobuf_c_message_get_packed_size ((const ProtobufCMessage*)(message));
}
size_t vaccel__torch__model_run_request__pack
                     (const Vaccel__Torch__ModelRunRequest *message,
                      uint8_t       *out)
{
  assert(message->base.descriptor == &vaccel__torch__model_run_request__descriptor);
  return protobuf_c_message_pack ((const ProtobufCMessage*)message, out);
}
size_t vaccel__torch__model_run_request__pack_to_buffer
                     (const Vaccel__Torch__ModelRunRequest *message,
                      ProtobufCBuffer *buffer)
{
  assert(message->base.descriptor == &vaccel__torch__model_run_request__descriptor);
  return protobuf_c_message_pack_to_buffer ((const ProtobufCMessage*)message, buffer);
}
Vaccel__Torch__ModelRunRequest *
       vaccel__torch__model_run_request__unpack
                     (ProtobufCAllocator  *allocator,
                      size_t               len,
                      const uint8_t       *data)
{
  return (Vaccel__Torch__ModelRunRequest *)
     protobuf_c_message_unpack (&vaccel__torch__model_run_request__descriptor,
                                allocator, len, data);
}
void   vaccel__torch__model_run_request__free_unpacked
                     (Vaccel__Torch__ModelRunRequest *message,
                      ProtobufCAllocator *allocator)
{
  if(!message)
    return;
  assert(message->base.descriptor == &vaccel__torch__model_run_request__descriptor);
  protobuf_c_message_free_unpacked ((ProtobufCMessage*)message, allocator);
}
void   vaccel__torch__model_run_response__init
                     (Vaccel__Torch__ModelRunResponse         *message)
{
  static const Vaccel__Torch__ModelRunResponse init_value = VACCEL__TORCH__MODEL_RUN_RESPONSE__INIT;
  *message = init_value;
}
size_t vaccel__torch__model_run_response__get_packed_size
                     (const Vaccel__Torch__ModelRunResponse *message)
{
  assert(message->base.descriptor == &vaccel__torch__model_run_response__descriptor);
  return protobuf_c_message_get_packed_size ((const ProtobufCMessage*)(message));
}
size_t vaccel__torch__model_run_response__pack
                     (const Vaccel__Torch__ModelRunResponse *message,
                      uint8_t       *out)
{
  assert(message->base.descriptor == &vaccel__torch__model_run_response__descriptor);
  return protobuf_c_message_pack ((const ProtobufCMessage*)message, out);
}
size_t vaccel__torch__model_run_response__pack_to_buffer
                     (const Vaccel__Torch__ModelRunResponse *message,
                      ProtobufCBuffer *buffer)
{
  assert(message->base.descriptor == &vaccel__torch__model_run_response__descriptor);
  return protobuf_c_message_pack_to_buffer ((const ProtobufCMessage*)message, buffer);
}
Vaccel__Torch__ModelRunResponse *
       vaccel__torch__model_run_response__unpack
                     (ProtobufCAllocator  *allocator,
                      size_t               len,
                      const uint8_t       *data)
{
  return (Vaccel__Torch__ModelRunResponse *)
     protobuf_c_message_unpack (&vaccel__torch__model_run_response__descriptor,
                                allocator, len, data);
}
void   vaccel__torch__model_run_response__free_unpacked
                     (Vaccel__Torch__ModelRunResponse *message,
                      ProtobufCAllocator *allocator)
{
  if(!message)
    return;
  assert(message->base.descriptor == &vaccel__torch__model_run_response__descriptor);
  protobuf_c_message_free_unpacked ((ProtobufCMessage*)message, allocator);
}
static const ProtobufCFieldDescriptor vaccel__torch__tensor__field_descriptors[3] =
{
  {
    "data",
    1,
    PROTOBUF_C_LABEL_NONE,
    PROTOBUF_C_TYPE_BYTES,
    0,   /* quantifier_offset */
    offsetof(Vaccel__Torch__Tensor, data),
    NULL,
    NULL,
    0,             /* flags */
    0,NULL,NULL    /* reserved1,reserved2, etc */
  },
  {
    "dims",
    2,
    PROTOBUF_C_LABEL_REPEATED,
    PROTOBUF_C_TYPE_INT64,
    offsetof(Vaccel__Torch__Tensor, n_dims),
    offsetof(Vaccel__Torch__Tensor, dims),
    NULL,
    NULL,
    0 | PROTOBUF_C_FIELD_FLAG_PACKED,             /* flags */
    0,NULL,NULL    /* reserved1,reserved2, etc */
  },
  {
    "type",
    3,
    PROTOBUF_C_LABEL_NONE,
    PROTOBUF_C_TYPE_ENUM,
    0,   /* quantifier_offset */
    offsetof(Vaccel__Torch__Tensor, type),
    &vaccel__torch__data_type__descriptor,
    NULL,
    0,             /* flags */
    0,NULL,NULL    /* reserved1,reserved2, etc */
  },
};
static const unsigned vaccel__torch__tensor__field_indices_by_name[] = {
  0,   /* field[0] = data */
  1,   /* field[1] = dims */
  2,   /* field[2] = type */
};
static const ProtobufCIntRange vaccel__torch__tensor__number_ranges[1 + 1] =
{
  { 1, 0 },
  { 0, 3 }
};
const ProtobufCMessageDescriptor vaccel__torch__tensor__descriptor =
{
  PROTOBUF_C__MESSAGE_DESCRIPTOR_MAGIC,
  "vaccel.torch.Tensor",
  "Tensor",
  "Vaccel__Torch__Tensor",
  "vaccel.torch",
  sizeof(Vaccel__Torch__Tensor),
  3,
  vaccel__torch__tensor__field_descriptors,
  vaccel__torch__tensor__field_indices_by_name,
  1,  vaccel__torch__tensor__number_ranges,
  (ProtobufCMessageInit) vaccel__torch__tensor__init,
  NULL,NULL,NULL    /* reserved[123] */
};
static const ProtobufCFieldDescriptor vaccel__torch__model_load_request__field_descriptors[2] =
{
  {
    "session_id",
    1,
    PROTOBUF_C_LABEL_NONE,
    PROTOBUF_C_TYPE_INT64,
    0,   /* quantifier_offset */
    offsetof(Vaccel__Torch__ModelLoadRequest, session_id),
    NULL,
    NULL,
    0,             /* flags */
    0,NULL,NULL    /* reserved1,reserved2, etc */
  },
  {
    "model_id",
    2,
    PROTOBUF_C_LABEL_NONE,
    PROTOBUF_C_TYPE_INT64,
    0,   /* quantifier_offset */
    offsetof(Vaccel__Torch__ModelLoadRequest, model_id),
    NULL,
    NULL,
    0,             /* flags */
    0,NULL,NULL    /* reserved1,reserved2, etc */
  },
};
static const unsigned vaccel__torch__model_load_request__field_indices_by_name[] = {
  1,   /* field[1] = model_id */
  0,   /* field[0] = session_id */
};
static const ProtobufCIntRange vaccel__torch__model_load_request__number_ranges[1 + 1] =
{
  { 1, 0 },
  { 0, 2 }
};
const ProtobufCMessageDescriptor vaccel__torch__model_load_request__descriptor =
{
  PROTOBUF_C__MESSAGE_DESCRIPTOR_MAGIC,
  "vaccel.torch.ModelLoadRequest",
  "ModelLoadRequest",
  "Vaccel__Torch__ModelLoadRequest",
  "vaccel.torch",
  sizeof(Vaccel__Torch__ModelLoadRequest),
  2,
  vaccel__torch__model_load_request__field_descriptors,
  vaccel__torch__model_load_request__field_indices_by_name,
  1,  vaccel__torch__model_load_request__number_ranges,
  (ProtobufCMessageInit) vaccel__torch__model_load_request__init,
  NULL,NULL,NULL    /* reserved[123] */
};
static const ProtobufCFieldDescriptor vaccel__torch__model_run_request__field_descriptors[5] =
{
  {
    "session_id",
    1,
    PROTOBUF_C_LABEL_NONE,
    PROTOBUF_C_TYPE_INT64,
    0,   /* quantifier_offset */
    offsetof(Vaccel__Torch__ModelRunRequest, session_id),
    NULL,
    NULL,
    0,             /* flags */
    0,NULL,NULL    /* reserved1,reserved2, etc */
  },
  {
    "model_id",
    2,
    PROTOBUF_C_LABEL_NONE,
    PROTOBUF_C_TYPE_INT64,
    0,   /* quantifier_offset */
    offsetof(Vaccel__Torch__ModelRunRequest, model_id),
    NULL,
    NULL,
    0,             /* flags */
    0,NULL,NULL    /* reserved1,reserved2, etc */
  },
  {
    "run_options",
    3,
    PROTOBUF_C_LABEL_NONE,
    PROTOBUF_C_TYPE_BYTES,
    0,   /* quantifier_offset */
    offsetof(Vaccel__Torch__ModelRunRequest, run_options),
    NULL,
    NULL,
    0,             /* flags */
    0,NULL,NULL    /* reserved1,reserved2, etc */
  },
  {
    "in_tensors",
    4,
    PROTOBUF_C_LABEL_REPEATED,
    PROTOBUF_C_TYPE_MESSAGE,
    offsetof(Vaccel__Torch__ModelRunRequest, n_in_tensors),
    offsetof(Vaccel__Torch__ModelRunRequest, in_tensors),
    &vaccel__torch__tensor__descriptor,
    NULL,
    0,             /* flags */
    0,NULL,NULL    /* reserved1,reserved2, etc */
  },
  {
    "nr_out_tensors",
    5,
    PROTOBUF_C_LABEL_NONE,
    PROTOBUF_C_TYPE_UINT64,
    0,   /* quantifier_offset */
    offsetof(Vaccel__Torch__ModelRunRequest, nr_out_tensors),
    NULL,
    NULL,
    0,             /* flags */
    0,NULL,NULL    /* reserved1,reserved2, etc */
  },
};
static const unsigned vaccel__torch__model_run_request__field_indices_by_name[] = {
  3,   /* field[3] = in_tensors */
  1,   /* field[1] = model_id */
  4,   /* field[4] = nr_out_tensors */
  2,   /* field[2] = run_options */
  0,   /* field[0] = session_id */
};
static const ProtobufCIntRange vaccel__torch__model_run_request__number_ranges[1 + 1] =
{
  { 1, 0 },
  { 0, 5 }
};
const ProtobufCMessageDescriptor vaccel__torch__model_run_request__descriptor =
{
  PROTOBUF_C__MESSAGE_DESCRIPTOR_MAGIC,
  "vaccel.torch.ModelRunRequest",
  "ModelRunRequest",
  "Vaccel__Torch__ModelRunRequest",
  "vaccel.torch",
  sizeof(Vaccel__Torch__ModelRunRequest),
  5,
  vaccel__torch__model_run_request__field_descriptors,
  vaccel__torch__model_run_request__field_indices_by_name,
  1,  vaccel__torch__model_run_request__number_ranges,
  (ProtobufCMessageInit) vaccel__torch__model_run_request__init,
  NULL,NULL,NULL    /* reserved[123] */
};
static const ProtobufCFieldDescriptor vaccel__torch__model_run_response__field_descriptors[1] =
{
  {
    "out_tensors",
    1,
    PROTOBUF_C_LABEL_REPEATED,
    PROTOBUF_C_TYPE_MESSAGE,
    offsetof(Vaccel__Torch__ModelRunResponse, n_out_tensors),
    offsetof(Vaccel__Torch__ModelRunResponse, out_tensors),
    &vaccel__torch__tensor__descriptor,
    NULL,
    0,             /* flags */
    0,NULL,NULL    /* reserved1,reserved2, etc */
  },
};
static const unsigned vaccel__torch__model_run_response__field_indices_by_name[] = {
  0,   /* field[0] = out_tensors */
};
static const ProtobufCIntRange vaccel__torch__model_run_response__number_ranges[1 + 1] =
{
  { 1, 0 },
  { 0, 1 }
};
const ProtobufCMessageDescriptor vaccel__torch__model_run_response__descriptor =
{
  PROTOBUF_C__MESSAGE_DESCRIPTOR_MAGIC,
  "vaccel.torch.ModelRunResponse",
  "ModelRunResponse",
  "Vaccel__Torch__ModelRunResponse",
  "vaccel.torch",
  sizeof(Vaccel__Torch__ModelRunResponse),
  1,
  vaccel__torch__model_run_response__field_descriptors,
  vaccel__torch__model_run_response__field_indices_by_name,
  1,  vaccel__torch__model_run_response__number_ranges,
  (ProtobufCMessageInit) vaccel__torch__model_run_response__init,
  NULL,NULL,NULL    /* reserved[123] */
};
static const ProtobufCEnumValue vaccel__torch__data_type__enum_values_by_number[8] =
{
  { "UNUSED", "VACCEL__TORCH__DATA_TYPE__UNUSED", 0 },
  { "BYTE", "VACCEL__TORCH__DATA_TYPE__BYTE", 1 },
  { "CHAR", "VACCEL__TORCH__DATA_TYPE__CHAR", 2 },
  { "SHORT", "VACCEL__TORCH__DATA_TYPE__SHORT", 3 },
  { "INT", "VACCEL__TORCH__DATA_TYPE__INT", 4 },
  { "LONG", "VACCEL__TORCH__DATA_TYPE__LONG", 5 },
  { "HALF", "VACCEL__TORCH__DATA_TYPE__HALF", 6 },
  { "FLOAT", "VACCEL__TORCH__DATA_TYPE__FLOAT", 7 },
};
static const ProtobufCIntRange vaccel__torch__data_type__value_ranges[] = {
{0, 0},{0, 8}
};
static const ProtobufCEnumValueIndex vaccel__torch__data_type__enum_values_by_name[8] =
{
  { "BYTE", 1 },
  { "CHAR", 2 },
  { "FLOAT", 7 },
  { "HALF", 6 },
  { "INT", 4 },
  { "LONG", 5 },
  { "SHORT", 3 },
  { "UNUSED", 0 },
};
const ProtobufCEnumDescriptor vaccel__torch__data_type__descriptor =
{
  PROTOBUF_C__ENUM_DESCRIPTOR_MAGIC,
  "vaccel.torch.DataType",
  "DataType",
  "Vaccel__Torch__DataType",
  "vaccel.torch",
  8,
  vaccel__torch__data_type__enum_values_by_number,
  8,
  vaccel__torch__data_type__enum_values_by_name,
  1,
  vaccel__torch__data_type__value_ranges,
  NULL,NULL,NULL,NULL   /* reserved[1234] */
};
